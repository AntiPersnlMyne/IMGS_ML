{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jACkm11KgPu"
      },
      "source": [
        "## Install **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BtAXkKTHKkAi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "## Download packages ##\n",
        "# -q means \"quiet\", which suppresses output\n",
        "%pip install -q opencv-python # Img. proc and video\n",
        "%pip install -q tensorflow    # Machine learning library\n",
        "%pip install -q pandas        # Dataframe/tables\n",
        "%pip install -q matplotlib    # Plotting\n",
        "%pip install -q openpyxl      # Excel support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdngRTJ0Kik9"
      },
      "source": [
        "Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TWTHlUXf4Sw",
        "outputId": "a94d3260-bf87-4875-eb1b-761701561602"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-23 22:12:20.154513: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-23 22:12:20.156652: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-23 22:12:20.188038: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-23 22:12:20.188983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-04-23 22:12:20.785072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Imports Successful\n"
          ]
        }
      ],
      "source": [
        "## Machine leraning imports ##\n",
        "\n",
        "# Language of Gods\n",
        "import tensorflow as tf\n",
        "# high-level API within TensorFlow that makes it easier to define and work with neural networks\n",
        "import tensorflow.keras as keras # type: ignore\n",
        "# This provides tools for creating different types of neural network models, like Sequential and Model\n",
        "from tensorflow.keras.models import Sequential, Model # type: ignore\n",
        "# Contains functions for preprocessing images, such as resizing, data augmentation, etc\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
        "# Building blocks for creating layers within neural networks\n",
        "from tensorflow.keras.layers import Concatenate, LSTM, GlobalAveragePooling2D, MaxPooling2D, BatchNormalization, Conv2D, Conv3D, Dense, Dropout, Flatten, TimeDistributed # type: ignore\n",
        "# Provides algorithms for optimizing the training process of a model\n",
        "from tensorflow.keras.optimizers import Adam # type: ignore\n",
        "# Includes functions to evaluate model performance\n",
        "from tensorflow.keras.metrics import categorical_crossentropy # type: ignore\n",
        "# Provides tools to prevent overfitting during training\n",
        "from tensorflow.keras import regularizers, layers, Input, Model # type: ignore\n",
        "# Slap categories from codes onto data\n",
        "from tensorflow.keras.utils import to_categorical # type: ignore\n",
        "# Optimize learning\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # type: ignore\n",
        "# This provides tools for splitting data into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "## Misc imports ##\n",
        "# Image loading or something\n",
        "import glob\n",
        "# Importing images from local directory\n",
        "from pathlib import Path\n",
        "# Creating directories or checking file paths.cnn\n",
        "import os\n",
        "# Number operations\n",
        "import numpy as np\n",
        "# Image read helper. OpenCV could also be used.\n",
        "from PIL import Image\n",
        "# Plot within the Colab notebook\n",
        "%matplotlib inline\n",
        "# Carl-related stuffs\n",
        "import cv2 as cv\n",
        "# Dataframes for adding classification labels and reading Excel (.csv) datasets\n",
        "import pandas as pd\n",
        "\n",
        "print(\"All Imports Successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd17vzJbQIDZ"
      },
      "source": [
        "## Import **Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUYak2c8qPSB"
      },
      "source": [
        "**Datasets used**\n",
        "1. MMEW, CASME II, SAMM (in progres of obtaining)\n",
        "2. SAMMv2 (ripped from Kaggle, stored in mt9485/Datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q6dfB6mM8t4U"
      },
      "outputs": [],
      "source": [
        "# Create class holding dataset\n",
        "class Dataset:\n",
        "    def __init__(self, root_dir, label_file, target_size=(128, 128),\n",
        "                 sequence_length=50, skiprows=0, dataframe=None, augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.target_size = target_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.augment = augment\n",
        "\n",
        "        # Load and clean the label file\n",
        "        if dataframe is not None:\n",
        "            self.labels_df = dataframe.reset_index(drop=True)\n",
        "        else:\n",
        "            # Read from Excel\n",
        "            self.labels_df = pd.read_excel(label_file, skiprows=skiprows)\n",
        "            self.labels_df = self.labels_df.dropna(subset=[\"Filename\", \"Onset Frame\", \"Offset Frame\", \"Objective Classes\"])\n",
        "            self.labels_df[\"Subject\"] = self.labels_df[\"Subject\"].astype(str).str.zfill(3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def _load_sequence(self, subject_id, clip_name, onset, offset):\n",
        "        clip_path = os.path.join(self.root_dir, subject_id, clip_name)\n",
        "\n",
        "        if not os.path.isdir(clip_path):\n",
        "            raise ValueError(f\"Clip folder does not exist: {clip_path}\")\n",
        "\n",
        "        frames = []\n",
        "\n",
        "        for frame_num in range(onset, offset + 1):\n",
        "            frame_str = str(frame_num).zfill(5)\n",
        "            \n",
        "            # Match any file ending in the frame number (4 or 5 digits)\n",
        "            pattern_1 = os.path.join(clip_path, f\"{clip_name}_*{frame_num}.jpg\")\n",
        "            pattern_2 = os.path.join(clip_path, f\"{subject_id}_*{frame_num}.jpg\")\n",
        "            \n",
        "            matches = glob.glob(pattern_1) + glob.glob(pattern_2)\n",
        "            if not matches:\n",
        "                continue  # No match found for this frame\n",
        "\n",
        "            # Use first matching file (usually only one)\n",
        "            img_path = matches[0]\n",
        "\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"L\")\n",
        "                img = img.resize(self.target_size)\n",
        "                img = np.array(img).astype(np.uint8)\n",
        "\n",
        "                if self.augment:\n",
        "                    img = self.apply_augmentation(img)\n",
        "\n",
        "                img = img.astype('float32') / 255.0\n",
        "                \n",
        "                frames.append(img)\n",
        "            except Exception as e:\n",
        "                print(f\"[!] Error loading image {img_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not frames:\n",
        "            raise ValueError(f\"No valid frames in: {clip_path} for range {onset}-{offset}\")\n",
        "\n",
        "        sequence = np.stack(frames, axis=0)\n",
        "        sequence = self._pad_or_truncate(sequence)\n",
        "\n",
        "        return sequence[..., np.newaxis]  # (frames, H, W, 1)\n",
        "\n",
        "\n",
        "    def _pad_or_truncate(self, sequence):\n",
        "        num_frames = sequence.shape[0]\n",
        "        if num_frames == self.sequence_length:\n",
        "            return sequence\n",
        "        elif num_frames < self.sequence_length:\n",
        "            pad_len = self.sequence_length - num_frames\n",
        "            pad = np.zeros((pad_len, *sequence.shape[1:]), dtype=sequence.dtype)\n",
        "            return np.concatenate([sequence, pad], axis=0)\n",
        "        else:\n",
        "            return sequence[:self.sequence_length]\n",
        "\n",
        "    def get_dataset(self):\n",
        "        sequences = []\n",
        "        labels = []\n",
        "\n",
        "        for idx, row in self.labels_df.iterrows():\n",
        "            try:\n",
        "                subject = row[\"Subject\"]\n",
        "                filename = row[\"Filename\"]\n",
        "                onset = int(row[\"Onset Frame\"])\n",
        "                offset = int(row[\"Offset Frame\"])\n",
        "                label = label = int(row[\"Objective Classes\"]) - 1  # Shift 1–7 → 0–6\n",
        "\n",
        "                sequence = self._load_sequence(subject, filename, onset, offset)\n",
        "                sequences.append(sequence)\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"[!] Skipped row {idx} due to error: {e}\")\n",
        "\n",
        "        x = np.array(sequences)\n",
        "        y = to_categorical(labels, num_classes=7)\n",
        "        \n",
        "        # print(f\"Min label: {min(labels)}, Max label: {max(labels)}\")\n",
        "\n",
        "        return x, y\n",
        "    \n",
        "    @staticmethod\n",
        "    def split_face_regions(sequence_batch):\n",
        "        \"\"\"\n",
        "        Takes in a batch of image sequences \n",
        "        and returns two batches:\n",
        "        - eye_seq: top half (N, T, 64, 128, 1)\n",
        "        - mouth_seq: bottom half (N, T, 64, 128, 1)\n",
        "        - N: Number of sequences\n",
        "        - T: Number of frame in each sequence\n",
        "        - H: Original height of each frame\n",
        "        - W: Original width of each frame\n",
        "        - C: Number of channels\n",
        "        \"\"\"\n",
        "        top_seqs = []\n",
        "        bottom_seqs = []\n",
        "\n",
        "        for seq in sequence_batch:\n",
        "            top_seq = []\n",
        "            bottom_seq = []\n",
        "\n",
        "            for frame in seq:\n",
        "                frame = frame.squeeze()  # (H, W)\n",
        "                h, w = frame.shape\n",
        "                \n",
        "                # Ensure height is even before splitting\n",
        "                if h % 2 != 0:\n",
        "                    frame = frame[:h-1, :]  # Crop one row if height is odd\n",
        "                    h -= 1\n",
        "\n",
        "                top = frame[:h//2, :]      # (H/2, W)\n",
        "                bottom = frame[h//2:, :]   # (H/2, W)\n",
        "\n",
        "                top_seq.append(top)\n",
        "                bottom_seq.append(bottom)\n",
        "\n",
        "            # Add channel dim back\n",
        "            top_seq = np.expand_dims(np.array(top_seq), -1)\n",
        "            bottom_seq = np.expand_dims(np.array(bottom_seq), -1)\n",
        "\n",
        "            top_seqs.append(top_seq)\n",
        "            bottom_seqs.append(bottom_seq)\n",
        "\n",
        "        return np.array(top_seqs), np.array(bottom_seqs)\n",
        "    \n",
        "    @staticmethod\n",
        "    def apply_augmentation(img):\n",
        "        \"\"\"Apply random augmentations to a grayscale image\"\"\"\n",
        "        # Horizontal flip (50%)\n",
        "        if np.random.rand() < 0.5:\n",
        "            img = np.flip(img, axis=1)\n",
        "\n",
        "        # Random brightness & contrast\n",
        "        alpha = np.random.uniform(0.9, 1.1)\n",
        "        beta = np.random.uniform(-10, 10)\n",
        "        img = np.clip(alpha * img + beta, 0, 255)\n",
        "\n",
        "        # Gaussian noise (very light)\n",
        "        if np.random.rand() < 0.25:\n",
        "            noise = np.random.normal(0, 3, img.shape)\n",
        "            img = np.clip(img + noise, 0, 255)\n",
        "\n",
        "        # Slight zoom (random crop & resize)\n",
        "        if np.random.rand() < 0.25:\n",
        "            h, w = img.shape\n",
        "            zoom_factor = np.random.uniform(1.0, 1.05)\n",
        "            zh, zw = int(h / zoom_factor), int(w / zoom_factor)\n",
        "            top = (h - zh) // 2\n",
        "            left = (w - zw) // 2\n",
        "            cropped = img[top:top+zh, left:left+zw]\n",
        "            img = cv.resize(cropped, (w, h))\n",
        "\n",
        "        return img.astype(np.uint8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88P0ap02A5wq",
        "outputId": "bf10d6f4-d945-4cb0-f37a-036c362c14de"
      },
      "outputs": [],
      "source": [
        "## Load SAMM Dataset ##\n",
        "\n",
        "# Load the full label file\n",
        "full_df = pd.read_excel(\"./SAMM_Micro_FACS_Codes_v2.xlsx\", skiprows=13)\n",
        "full_df = full_df.dropna(subset=[\"Filename\", \"Onset Frame\", \"Offset Frame\", \"Objective Classes\"])\n",
        "full_df[\"Subject\"] = full_df[\"Subject\"].astype(str).str.zfill(3)\n",
        "\n",
        "# Split training and val into separate pieces; one to augment, other not\n",
        "train_df, val_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.2,\n",
        "    stratify=full_df[\"Objective Classes\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Local dataset path\n",
        "local_dataset_path = \"./SAMMv2\"\n",
        "\n",
        "# Instantiate dataset \n",
        "train_dataset = Dataset(\n",
        "    root_dir=local_dataset_path,\n",
        "    label_file=\"./SAMM_Micro_FACS_Codes_v2.xlsx\",  # Still needed in case dataframe is None\n",
        "    target_size=(128, 128),\n",
        "    sequence_length=50,\n",
        "    dataframe=train_df,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset = Dataset(\n",
        "    root_dir=local_dataset_path,\n",
        "    label_file=\"./SAMM_Micro_FACS_Codes_v2.xlsx\",\n",
        "    target_size=(128, 128),\n",
        "    sequence_length=50,\n",
        "    dataframe=val_df,\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "# Load the data (80% train, 20% val)\n",
        "SAMMv2_x_train, SAMMv2_y_train = train_dataset.get_dataset()\n",
        "SAMMv2_x_val, SAMMv2_y_val = val_dataset.get_dataset()\n",
        "\n",
        "# For your model\n",
        "SAMMv2_input_shape = (50, 128, 128, 1)\n",
        "SAMMv2_num_classes = 7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoOZqTnPGO3y"
      },
      "source": [
        "## Build Model **Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inception Block\n",
        "class InceptionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1x3 = Conv2D(num_channels, kernel_size=(1,3), padding='same', activation='relu')\n",
        "        self.conv3x1 = Conv2D(num_channels, kernel_size=(3,1), padding='same', activation='relu')\n",
        "        self.conv3x3 = Conv2D(num_channels, kernel_size=(3), padding='same', activation='relu')\n",
        "        self.conv1x1 = Conv2D(num_channels, kernel_size=(1), padding='same', activation='relu')\n",
        "        self.maxpool = MaxPooling2D(pool_size=(3,3), strides=1, padding='same')\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "      # path a\n",
        "        a = self.conv1x1(x)\n",
        "        a = self.conv3x3(a)\n",
        "      ## path a_1\n",
        "        a_1 = self.conv1x3(a)\n",
        "      ## path a_2\n",
        "        a_2 = self.conv3x1(a)\n",
        "\n",
        "      # path b\n",
        "        b = self.conv1x1(x)\n",
        "      ## path b_1\n",
        "        b_1 = self.conv1x3(b)\n",
        "      ## path b_2\n",
        "        b_2 = self.conv3x1(b)\n",
        "\n",
        "      # path c\n",
        "        c = self.maxpool(x)\n",
        "        c = self.conv1x1(c)\n",
        "\n",
        "      # path d\n",
        "        d = self.conv1x1(x)\n",
        "\n",
        "        return tf.keras.layers.concatenate([a, a_1, a_2, b, b_1, b_2, c, d])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r70evmsADODw"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels=64):\n",
        "        super().__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            InceptionBlock(num_channels),\n",
        "            Conv2D(num_channels, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Conv2D(num_channels, kernel_size=3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.1),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top and Bottom half branches. To Process eye and mouth data separately.\n",
        "class CNN_LSTM_Branch(tf.keras.Model):\n",
        "    def __init__(self, num_channels=64, lstm_units=128, name=\"cnn_lstm_branch\"):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        # TimeDistributed applies CNNBlock to each frame\n",
        "        self.frames = TimeDistributed(CNNBlock(num_channels))\n",
        "        \n",
        "        # Flatten output for dense layer\n",
        "        self.flatten = Flatten()\n",
        "        \n",
        "        # Apply LSTM\n",
        "        self.lstm = tf.keras.layers.ConvLSTM2D(lstm_units, kernel_size=(3), padding='same')\n",
        "\n",
        "        \n",
        "    def call(self, x):\n",
        "        x = self.frames(x)  # Process frames\n",
        "        x = self.lstm(x) # Flatten output\n",
        "        x = self.flatten(x)    # Apply LSTSM\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalize layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Instantiate and combine branches ##\n",
        "# Inputs (#frames, img_width, img_height, channels)\n",
        "eye_input = tf.keras.Input(shape=(50, 64, 128, 1), name=\"eye_input\")\n",
        "mouth_input = tf.keras.Input(shape=(50, 64, 128, 1), name=\"mouth_input\")\n",
        "\n",
        "# Top and Bottom branch processes eyes (top) and mouth (bottom), halves of the image\n",
        "#  Need to be named uniquely, or else gets angry\n",
        "eye_branch = CNN_LSTM_Branch(name=\"eye_branch\")\n",
        "mouth_branch = CNN_LSTM_Branch(name=\"mouth_branch\")\n",
        "\n",
        "# Call the model on the inputs — this builds the graph\n",
        "eye_output = eye_branch(eye_input)\n",
        "mouth_output = mouth_branch(mouth_input)\n",
        "\n",
        "\n",
        "\n",
        "## Dense layers for classification ##\n",
        "concatination = tf.keras.layers.Concatenate()([eye_output, mouth_output])\n",
        "\n",
        "x = Flatten()(concatination)\n",
        "\n",
        "x = Dense(128, activation='relu')(concatination)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "# Classification - 7 classes = 7 emotions\n",
        "cnnlstm_output = tf.keras.layers.Dense(7, activation='softmax')(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_48uU75Gu7w"
      },
      "source": [
        "## **Construct and Test** Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Split the data ## \n",
        "SAMMv2_eye_train, SAMMv2_mouth_train = Dataset.split_face_regions(SAMMv2_x_train)\n",
        "SAMMv2_eye_val, SAMMv2_mouth_val = Dataset.split_face_regions(SAMMv2_x_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Y3HyiGcXGh-G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " eye_input (InputLayer)         [(None, 50, 64, 128  0           []                               \n",
            "                                , 1)]                                                             \n",
            "                                                                                                  \n",
            " mouth_input (InputLayer)       [(None, 50, 64, 128  0           []                               \n",
            "                                , 1)]                                                             \n",
            "                                                                                                  \n",
            " eye_branch (CNN_LSTM_Branch)   (None, 1048576)      1279424     ['eye_input[0][0]']              \n",
            "                                                                                                  \n",
            " mouth_branch (CNN_LSTM_Branch)  (None, 1048576)     1279424     ['mouth_input[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2097152)      0           ['eye_branch[0][0]',             \n",
            "                                                                  'mouth_branch[0][0]']           \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          268435584   ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           8256        ['dropout_42[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 64)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 32)           2080        ['dropout_43[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 7)            231         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 271,004,999\n",
            "Trainable params: 271,004,487\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "(127, 50, 64, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "# Store model into a variable\n",
        "cnnlstm = tf.keras.Model(inputs=[eye_input,mouth_input], outputs=cnnlstm_output)\n",
        "\n",
        "cnnlstm.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',   # not 'sparse' bcs using one-hot encoding\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks_cnnlstm = [\n",
        "    # Stops if the loss function has 3 consequtive dips in performance\n",
        "    EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    # Saves the best model after training\n",
        "    ModelCheckpoint(\"cnnlstm_best_model.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "cnnlstm.summary()\n",
        "\n",
        "print(SAMMv2_eye_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 221s 27s/step - loss: 77.3320 - accuracy: 0.2598 - val_loss: 65.5780 - val_accuracy: 0.5312\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 194s 24s/step - loss: 89.9413 - accuracy: 0.3150 - val_loss: 69.8464 - val_accuracy: 0.1562\n",
            "Epoch 3/50\n",
            "1/8 [==>...........................] - ETA: 2:39 - loss: 126.2550 - accuracy: 0.2500"
          ]
        }
      ],
      "source": [
        "## Training the Model ##\n",
        "cnnlstm.fit(\n",
        "    [SAMMv2_eye_train, SAMMv2_mouth_train], SAMMv2_y_train,\n",
        "    validation_data=([SAMMv2_eye_val, SAMMv2_mouth_val], SAMMv2_y_val),\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_cnnlstm\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7jACkm11KgPu",
        "UoOZqTnPGO3y",
        "AH1Ws_PGHVUA",
        "DDasvfACLSR_",
        "qQhX7fZJLZ3U"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
